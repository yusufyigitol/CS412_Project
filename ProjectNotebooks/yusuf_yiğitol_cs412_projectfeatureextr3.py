# -*- coding: utf-8 -*-
"""Yusuf-Yiğitol-Cs412-ProjectFeatureExtR3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-r2fqFjnC5guDxCnveFABizGGsceGoqe
"""

import numpy as np
import pandas as pd
import gzip
import json
import nltk
from nltk.corpus import stopwords
from pprint import pprint
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import re
import os

# Download Turkish Stopwords
nltk.download('stopwords')
turkish_stopwords = stopwords.words('turkish')

# Mount Google Drive if using Colab
try:
    from google.colab import drive
    drive.mount('/content/drive')
    base_path = "/content/drive/MyDrive/CS 412/Project/"
except ImportError:
    base_path = "./"

# Paths to datasets
train_classification_path = os.path.join(base_path, "train-classification.csv")
train_data_path = os.path.join(base_path, "training-dataset.jsonl.gz")
test_classification_path = os.path.join(base_path, "test-classification-round3.dat")
test_regression_path = os.path.join(base_path, "test-regression-round3.jsonl")

# Load Classification Dataset
train_classification_df = pd.read_csv(train_classification_path)
train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})
train_classification_df["category"] = train_classification_df["category"].apply(str.lower)
username2_category = train_classification_df.set_index("user_id").to_dict()["category"]

# Stats about labels
print(train_classification_df.groupby("category").count())

# Load Training Data
username2posts_train = dict()
username2profile_train = dict()
username2posts_test = dict()
username2profile_test = dict()

with gzip.open(train_data_path, "rt") as fh:
    for line in fh:
        sample = json.loads(line)
        profile = sample["profile"]
        username = profile["username"]
        if username in username2_category:
            username2posts_train[username] = sample["posts"]
            username2profile_train[username] = profile
        else:
            username2posts_test[username] = sample["posts"]
            username2profile_test[username] = profile

# Convert Profiles to DataFrame
train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)
test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)
print("Training profiles shape:", train_profile_df.shape)
print("Test profiles shape:", test_profile_df.shape)

# Text Preprocessing
def preprocess_text(text: str):
    text = text.casefold()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zçğıöşü0-9\s#@]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Prepare Corpus for TF-IDF
corpus = []
train_usernames = []
for username, posts in username2posts_train.items():
    train_usernames.append(username)
    cleaned_captions = []
    for post in posts:
        post_caption = post.get("caption", "")
        if post_caption:
            cleaned_captions.append(preprocess_text(post_caption))
    corpus.append("\n".join(cleaned_captions))

vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=10000, ngram_range=(1, 2))
vectorizer.fit(corpus)

# Transform Data
x_post_train = vectorizer.transform(corpus)
y_train = [username2_category[uname] for uname in train_usernames]

# Prepare Test Data
test_usernames = []
test_corpus = []
for username, posts in username2posts_test.items():
    test_usernames.append(username)
    cleaned_captions = []
    for post in posts:
        post_caption = post.get("caption", "")
        if post_caption:
            cleaned_captions.append(preprocess_text(post_caption))
    test_corpus.append("\n".join(cleaned_captions))

x_post_test = vectorizer.transform(test_corpus)

# Ensure no "NA" in y_train
assert y_train.count("NA") == 0

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV


# Train Logistic Regression Model with Cross-Validation
model = LogisticRegression(max_iter=1000, class_weight="balanced")
scores = cross_val_score(model, x_post_train, y_train, cv=5, scoring='accuracy')

# Print Cross-Validation Results
print("Cross-Validation Accuracy Scores:", scores)
print("Mean Accuracy:", scores.mean())

params = {'C': [0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(LogisticRegression(max_iter=1000, class_weight="balanced"), param_grid=params, cv=5)
grid.fit(x_post_train, y_train)

# Use the best model from GridSearch
model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# Fit the model on the full training data
model.fit(x_post_train, y_train)

# Predict Test Data
x_test_usernames = []
x_test_vectors = []

with open(test_classification_path, "r") as fh:
    for line in fh:
        username = line.strip()
        if username in test_usernames:
            x_test_vectors.append(x_post_test[test_usernames.index(username)].toarray()[0])
            x_test_usernames.append(username)

df_test = pd.DataFrame(np.array(x_test_vectors), columns=vectorizer.get_feature_names_out())

predictions = model.predict(x_test_vectors)
print(f"Predictions type: {type(predictions[0])}, Example: {predictions[:5]}")

output_classification = {uname: predictions[i] for i, uname in enumerate(x_test_usernames)}

output_classification_path = os.path.join(base_path, "prediction-classification-round3.json")
with open(output_classification_path, "w") as outfile:
    json.dump(output_classification, outfile)

print("Classification predictions saved.")

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
import numpy as np
import json
import os

# Prepare Data for Regression
like_counts = []
train_regression_features = []

for username, posts in username2posts_train.items():
    if posts:
        avg_like = sum(post.get("like_count", 0) or 0 for post in posts) / len(posts)
        total_likes = sum(post.get("like_count", 0) or 0 for post in posts)
        avg_caption_length = np.mean([len(post["caption"]) if post.get("caption") else 0 for post in posts])
    else:
        avg_like, total_likes, avg_caption_length = 0, 0, 0

    tfidf_vector = x_post_train[train_usernames.index(username)].toarray()[0]
    combined_features = np.concatenate(([avg_like, total_likes, avg_caption_length], tfidf_vector))
    train_regression_features.append(combined_features)
    like_counts.append(avg_like)

train_regression_features = np.array(train_regression_features)

# Train-Test Split for Regression
x_reg_train, x_reg_val, y_reg_train, y_reg_val = train_test_split(
    train_regression_features, like_counts, test_size=0.2, random_state=42
)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV

param_distributions = {
    "n_estimators": [50, 100, 200],
    "max_depth": [3, 5],
    "learning_rate": [0.01, 0.1],
}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_distributions=param_distributions,
    n_iter=8,
    cv=5,
    scoring="r2",
    verbose=2,
    n_jobs=-1,
)

random_search.fit(x_reg_train, y_reg_train)

print("Best Parameters:", random_search.best_params_)
print("Best Score:", random_search.best_score_)

best_regressor = random_search.best_estimator_

# Process Test Regression Data
output_regression = []
output_regression_path = os.path.join(base_path, "prediction-regression-round3.json")

with open(test_regression_path, "r") as fh:
    for line in fh:
        try:
            sample = json.loads(line.strip())

            if "username" in sample and sample["username"] in test_usernames:
                username = sample["username"]

                # Compute features for the username
                if username in test_usernames:
                    try:
                        feature_vector = x_post_test[test_usernames.index(username)].toarray()[0]

                        # Add additional features for prediction
                        avg_like = sum(post.get("like_count", 0) or 0 for post in username2posts_test[username]) / len(
                            username2posts_test[username]
                        ) if username2posts_test[username] else 0
                        total_likes = sum(post.get("like_count", 0) or 0 for post in username2posts_test[username])
                        avg_caption_length = np.mean(
                            [len(post["caption"]) if post.get("caption") else 0 for post in username2posts_test[username]]
                        ) if username2posts_test[username] else 0

                        # Combine features for prediction
                        combined_features = np.concatenate(([avg_like, total_likes, avg_caption_length], feature_vector))

                        # Predict like count
                        predicted_like = best_regressor.predict([combined_features])[0]
                        sample["like_count"] = max(0, int(predicted_like))
                    except Exception as e:
                        print(f"Prediction failed for username {username}: {e}")
                        sample["like_count"] = 0
                else:
                    sample["like_count"] = 0  # Default if username not found
            else:
                sample["like_count"] = 0  # Default for missing usernames

            output_regression.append(sample)
        except Exception as e:
            print(f"Error processing line: {line.strip()}, Error: {e}")

# Simplify Regression Output
simplified_regression_output = {sample["id"]: sample["like_count"] for sample in output_regression if "id" in sample}

# Save Simplified Regression Predictions
with open(output_regression_path, "w") as outfile:
    json.dump(simplified_regression_output, outfile)

print(f"Regression predictions saved to {output_regression_path}.")